{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc6rDVZgnLDK"
      },
      "source": [
        "# Shoplite RAG System - Complete Deployment\n",
        "\n",
        "**Week 3 Assignment - Joseph Chamoun**\n",
        "\n",
        "This notebook is fully self-contained and implements a complete RAG system for Shoplite customer service.\n",
        "\n",
        "## Features:\n",
        "- ✅ 15 embedded knowledge base documents\n",
        "- ✅ FAISS vector search with sentence-transformers\n",
        "- ✅ Llama 3.1 8B (or fallback to smaller model)\n",
        "- ✅ Structured YAML prompts integrated\n",
        "- ✅ Flask API with /chat, /ping, /health endpoints\n",
        "- ✅ ngrok tunnel with runtime token input\n",
        "- ✅ Smart retrieval with confidence scoring\n",
        "\n",
        "**IMPORTANT**: Make sure to select GPU runtime (Runtime → Change runtime type → GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RGZOzzgnLDQ"
      },
      "source": [
        "# Cell 1: Install all dependencies\n",
        "print(\"📦 Installing dependencies... (this may take 2-3 minutes)\")\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q transformers accelerate bitsandbytes sentence-transformers faiss-cpu flask pyngrok\n",
        "print(\"✅ Installation complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuEInpzSnLDT"
      },
      "source": [
        "# Cell 2: Imports\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import threading\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import torch\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "\n",
        "print(\"✅ All imports successful\")\n",
        "print(f\"🔥 CUDA available: {torch.cuda.is_available()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "effDC2QanLDU"
      },
      "source": [
        "# Cell 3: Knowledge Base (15 Shoplite documents embedded)\n",
        "KNOWLEDGE_BASE = [\n",
        "    {\n",
        "        \"id\": \"doc1\",\n",
        "        \"title\": \"Shoplite User Registration and Account Management\",\n",
        "        \"content\": \"To create a Shoplite account, users must visit the registration page and provide a valid email address, password, and basic profile information. Email verification is required within 24 hours. Users can choose between buyer accounts (free) or seller accounts (requires business verification and tax information). Account Management Features include: Update personal information, Change passwords, Set security questions, Manage notification preferences, Deactivate accounts (requires email confirmation; may affect active orders/subscriptions). Buyer Access includes: product browsing, purchasing, order tracking, reviews. Seller Access includes: seller dashboard, inventory management, order processing, analytics. Security Measures: two-factor authentication recommended, password recovery via email and phone verification.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc2\",\n",
        "        \"title\": \"Shoplite Product Search and Filtering Features\",\n",
        "        \"content\": \"Shoplite provides a powerful search engine with keyword queries, category selection, and brand filters. Filtering Options include: price range, rating, availability, seller location, shipping speed, promotions, and eco-friendly options. Features include autocomplete suggestions, spelling correction, save searches and alerts, faceted navigation for combining multiple filters, optimization for large catalogs with real-time indexing, and a mobile responsive interface.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc3\",\n",
        "        \"title\": \"Shoplite Shopping Cart and Checkout Process\",\n",
        "        \"content\": \"Users can add multiple items from different sellers, review quantities, and apply promo codes or gift cards. The cart is preserved across sessions for logged-in users. Checkout Steps: 1) Shipping selection (standard, expedited, same-day), 2) Payment selection (credit/debit cards, digital wallets, cash-on-delivery), 3) Order confirmation. Security and Processing features include PCI-DSS compliant payment gateways, real-time stock updates, order confirmation emails with tracking, seller notifications for new orders, and an integrated returns and refunds system.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc4\",\n",
        "        \"title\": \"Shoplite Payment Methods and Security\",\n",
        "        \"content\": \"Accepted Payment Methods include credit/debit cards, PayPal, Apple Pay, Google Pay, and local payment solutions. Security Measures include SSL encryption, PCI-DSS compliance, fraud detection systems, two-factor authentication, and sensitive information encrypted both in transit and at rest. Other Features include digital wallet integration, a structured dispute and chargeback process, and seller payments processed after order confirmation.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc5\",\n",
        "        \"title\": \"Shoplite Order Tracking and Delivery\",\n",
        "        \"content\": \"Shoplite provides real-time tracking with confirmation emails and unique tracking numbers. Order Stages include: confirmed, processing, shipped, in transit, and delivered. Users can request delivery modifications (seller approval required). International shipments display customs and import duties information. The system uses optimized logistics with estimated arrival times and delay notifications. Support assistance is available for lost or delayed packages.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc6\",\n",
        "        \"title\": \"Shoplite Return and Refund Policies\",\n",
        "        \"content\": \"The return period is typically 30 days from delivery. Process: select the order and item, specify the reason, and use the prepaid label if eligible. Refunds are processed in 5–7 business days to the original payment method. Digital and personalized items may have exceptions. The system provides automated order status updates. Sellers must comply with return policies to maintain their ratings. Dispute resolution services are available for complex cases.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc7\",\n",
        "        \"title\": \"Shoplite Product Reviews and Ratings\",\n",
        "        \"content\": \"Buyers can rate products on a five-star scale and leave detailed comments. Reviews are moderated for compliance with community guidelines. Sellers can respond to reviews to address concerns. Ratings influence search ranking and product visibility. Verified purchase badges ensure review authenticity. Aggregate ratings are provided on product pages. Review analytics are available for sellers to track customer feedback trends.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc8\",\n",
        "        \"title\": \"Shoplite Seller Account Setup and Management\",\n",
        "        \"content\": \"To create a seller account, provide business documents and complete tax verification, which takes 2-3 business days. The Seller Dashboard includes inventory management, order processing, and sales analytics. Product listing is available via individual entry or bulk upload (CSV/API). Profile customization includes branding, policies, shipping options, and return policies. Sellers receive notifications for new orders, low stock alerts, and customer inquiries. Features include pricing management, promotional tools, and shipping fee configuration. Performance metrics are tracked, and third-party integrations are supported.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc9\",\n",
        "        \"title\": \"Shoplite Inventory Management for Sellers\",\n",
        "        \"content\": \"Sellers can track stock levels, set reorder thresholds, and manage availability in real-time. The system provides low-stock alerts to prevent stockouts. Bulk imports are supported for efficient inventory updates. Product variants (size, color, bundles) are fully supported. Inventory reports help identify trends and prepare for seasonal demand. Sellers can manage multiple warehouses and shipping locations from a single dashboard.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc10\",\n",
        "        \"title\": \"Shoplite Commission and Fee Structure\",\n",
        "        \"content\": \"Shoplite charges a standard 5% commission on each sale. Commission fees vary by product category. Additional fees may apply for premium listings, promotional campaigns, and special services. Transparent fee notifications are displayed in the seller dashboard. Payments are made after commission deduction on a weekly or bi-weekly schedule. Detailed transaction reports are available for accounting purposes. Pricing guidance helps sellers remain competitive.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc11\",\n",
        "        \"title\": \"Shoplite Customer Support Procedures\",\n",
        "        \"content\": \"Support is available via live chat, email, phone, and an AI chatbot, with 24/7 availability. Tickets are categorized by type: orders, payments, returns, technical issues, and account management. Each ticket receives a unique tracking ID. Backend integration provides instant access to order and payment information. A dedicated seller support channel addresses business-specific concerns. The help center includes comprehensive guides, FAQs, and video tutorials. The goal is fast, transparent, and fair resolution for all users.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc12\",\n",
        "        \"title\": \"Shoplite Mobile App Features\",\n",
        "        \"content\": \"The Shoplite mobile app supports iOS and Android devices. Users can browse products, apply filters, add items to cart, and complete purchases. Push notifications alert users to promotions and order updates. Features include barcode scanning and QR code payments for convenience. Mobile wallets, fingerprint authentication, and Face ID login enhance security. Sellers can manage their stores and process orders on-the-go. Offline caching allows users to view previously loaded content without an internet connection. The interface is intuitive, responsive, and accessible.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc13\",\n",
        "        \"title\": \"Shoplite API Documentation for Developers\",\n",
        "        \"content\": \"Shoplite provides RESTful API endpoints for product catalog access, order management, account operations, and inventory updates. Authentication uses OAuth 2.0 for secure access. Rate limiting applies to prevent abuse, with higher limits for verified partners. Detailed documentation includes request/response examples, parameter descriptions, and error code explanations. Webhooks enable real-time event notifications for orders, inventory changes, and payments. A sandbox environment is available for testing without affecting live data. The API is versioned to ensure backward-compatible updates.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc14\",\n",
        "        \"title\": \"Shoplite Security and Privacy Policies\",\n",
        "        \"content\": \"Data Protection includes TLS encryption for data in transit and AES-256 encryption for data at rest. Access is restricted to authorized personnel only. Two-factor authentication and strong password requirements protect user accounts. Shoplite complies with GDPR and CCPA regulations. Security monitoring detects suspicious activity in real-time. Clear privacy policies outline data collection, usage, and third-party sharing practices. Users are notified of policy changes and maintain control over their personal data, including the ability to download or delete their information.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc15\",\n",
        "        \"title\": \"Shoplite Promotional Codes and Discounts\",\n",
        "        \"content\": \"Sellers can create promotions including discount codes, seasonal sales, and bundle offers. Code types include percentage discounts, fixed amount discounts, and conditional discounts. Configuration options include start/end dates, usage limits, and minimum purchase requirements. Automatic verification occurs at checkout to ensure eligibility. Analytics track redemption rates, revenue impact, and customer engagement. Users receive notifications about active promotions they qualify for. Special events are highlighted on the homepage and mobile app. All promotions must comply with platform policies to ensure fairness.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"✅ Knowledge base loaded: {len(KNOWLEDGE_BASE)} documents\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox97pQV-nLDW"
      },
      "source": [
        "# Cell 4: Structured Prompts (converted from YAML)\n",
        "PROMPTS = {\n",
        "    \"version\": \"1.0\",\n",
        "    \"created\": \"2025-09-26\",\n",
        "    \"author\": \"Joseph Chamoun\",\n",
        "    \n",
        "    \"base_retrieval_prompt\": {\n",
        "        \"role\": \"You are a knowledgeable Shoplite customer service assistant.\",\n",
        "        \"goal\": \"Provide accurate, concise answers using only the provided Shoplite documentation.\",\n",
        "        \"context_guidelines\": [\n",
        "            \"Use only information from the provided document snippets\",\n",
        "            \"Cite specific documents when possible\",\n",
        "            \"Keep answers focused and relevant\",\n",
        "            \"If information is unclear, acknowledge limitations\"\n",
        "        ],\n",
        "        \"response_format\": \"Provide a clear, direct answer in 2-3 sentences based on the context.\"\n",
        "    },\n",
        "    \n",
        "    \"multi_doc_synthesis\": {\n",
        "        \"role\": \"You are an expert Shoplite support agent who synthesizes information from multiple documents.\",\n",
        "        \"goal\": \"Combine relevant information from multiple sources to create a comprehensive, accurate answer.\",\n",
        "        \"context_guidelines\": [\n",
        "            \"Retrieve and integrate information from all relevant documents\",\n",
        "            \"Provide step-by-step guidance if needed\",\n",
        "            \"Avoid adding information not present in the documents\",\n",
        "            \"Maintain consistency across different document sources\"\n",
        "        ],\n",
        "        \"response_format\": \"Synthesize information from multiple sources into a coherent answer.\"\n",
        "    },\n",
        "    \n",
        "    \"clarification_prompt\": {\n",
        "        \"role\": \"You are a helpful Shoplite assistant that seeks clarity when needed.\",\n",
        "        \"goal\": \"Ask for clarification politely when the user query is unclear or insufficient.\",\n",
        "        \"context_guidelines\": [\n",
        "            \"Do not guess answers if the query is unclear\",\n",
        "            \"Suggest specific questions or information needed\",\n",
        "            \"Remain helpful and professional\"\n",
        "        ],\n",
        "        \"response_format\": \"Politely ask for clarification with specific guidance.\"\n",
        "    },\n",
        "    \n",
        "    \"refusal_prompt\": {\n",
        "        \"role\": \"You are a responsible Shoplite assistant that only answers when relevant context is available.\",\n",
        "        \"goal\": \"Politely refuse to answer if the requested information is not found in the knowledge base.\",\n",
        "        \"context_guidelines\": [\n",
        "            \"Do not hallucinate information\",\n",
        "            \"Provide guidance on where the user may find help\",\n",
        "            \"Remain professional and helpful\"\n",
        "        ],\n",
        "        \"response_format\": \"Politely explain that the information is not available and suggest alternatives.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"✅ Loaded {len(PROMPTS) - 3} structured prompt configurations\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9zwZ3MlnLDX"
      },
      "source": [
        "# Cell 5: Build FAISS Index with Embeddings\n",
        "print(\"🔄 Loading embedding model...\")\n",
        "EMBED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "\n",
        "# Prepare documents for embedding\n",
        "DOCUMENT_TEXTS = [f\"{d['title']}\\n\\n{d['content']}\" for d in KNOWLEDGE_BASE]\n",
        "DOC_IDS = [d['id'] for d in KNOWLEDGE_BASE]\n",
        "\n",
        "print(\"🔄 Creating embeddings...\")\n",
        "doc_embeddings = embed_model.encode(DOCUMENT_TEXTS, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "# Normalize for cosine similarity\n",
        "def normalize_embeddings(embs):\n",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
        "    return embs / np.clip(norms, a_min=1e-10, a_max=None)\n",
        "\n",
        "doc_embeddings = normalize_embeddings(doc_embeddings)\n",
        "\n",
        "# Build FAISS index\n",
        "dimension = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
        "index.add(doc_embeddings)\n",
        "\n",
        "print(f\"✅ FAISS index built: {index.ntotal} vectors (dim={dimension})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoLI92xvnLDX"
      },
      "source": [
        "# Cell 6: Retrieval Function\n",
        "def retrieve_docs(query: str, top_k: int = 3):\n",
        "    \"\"\"Retrieve most relevant documents for a query.\"\"\"\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
        "    q_emb = normalize_embeddings(q_emb)\n",
        "    \n",
        "    scores, indices = index.search(q_emb, top_k)\n",
        "    \n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], indices[0]):\n",
        "        if 0 <= idx < len(KNOWLEDGE_BASE):\n",
        "            doc = KNOWLEDGE_BASE[idx]\n",
        "            results.append({\n",
        "                'id': doc['id'],\n",
        "                'title': doc['title'],\n",
        "                'content': doc['content'],\n",
        "                'score': float(score)\n",
        "            })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Quick test\n",
        "test_results = retrieve_docs(\"How do I create a seller account?\", top_k=3)\n",
        "print(f\"✅ Retrieval test passed: Found {len(test_results)} relevant docs\")\n",
        "print(f\"   Top match: {test_results[0]['title']} (score: {test_results[0]['score']:.3f})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfyCXmAPnLDY"
      },
      "source": [
        "# Cell 7: Load LLM with Quantization\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "FALLBACK_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"  # Smaller fallback\n",
        "\n",
        "print(f\"🔄 Loading model: {MODEL_NAME}\")\n",
        "print(f\"💡 Fallback available: {FALLBACK_MODEL}\")\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"{'🔥' if use_cuda else '⚠️'} CUDA available: {use_cuda}\")\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "loaded_model_name = None\n",
        "\n",
        "def try_load_model(model_name: str):\n",
        "    \"\"\"Attempt to load model with 8-bit quantization.\"\"\"\n",
        "    try:\n",
        "        print(f\"  Attempting: {model_name}\")\n",
        "        \n",
        "        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        \n",
        "        # Configure 8-bit quantization\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "        \n",
        "        mdl = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\" if use_cuda else None,\n",
        "            quantization_config=bnb_config if use_cuda else None,\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        \n",
        "        mdl.eval()\n",
        "        print(f\"  ✅ Success!\")\n",
        "        return tok, mdl, model_name\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Failed: {str(e)[:100]}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Try primary model\n",
        "tokenizer, model, loaded_model_name = try_load_model(MODEL_NAME)\n",
        "\n",
        "# Try fallback if primary fails\n",
        "if model is None:\n",
        "    print(f\"\\n🔄 Trying fallback model...\")\n",
        "    tokenizer, model, loaded_model_name = try_load_model(FALLBACK_MODEL)\n",
        "\n",
        "# Final status\n",
        "if model is not None:\n",
        "    print(f\"\\n✅ MODEL LOADED: {loaded_model_name}\")\n",
        "    print(f\"   Memory: ~{torch.cuda.memory_allocated() / 1e9:.1f} GB\" if use_cuda else \"   Running on CPU\")\n",
        "else:\n",
        "    print(\"\\n⚠️ No model loaded - will use extractive responses from retrieved docs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyO4x8ZQnLDY"
      },
      "source": [
        "# Cell 8: Complete Generation Function with Structured Prompts\n",
        "SIMILARITY_THRESHOLD = 0.35\n",
        "MAX_NEW_TOKENS = 250\n",
        "TEMPERATURE = 0.7\n",
        "\n",
        "def build_prompt_from_retrieval(query: str, retrieved_docs: List[Dict[str, Any]], prompt_type: str = \"base_retrieval_prompt\"):\n",
        "    \"\"\"Build prompt using structured YAML prompts.\"\"\"\n",
        "    # Get prompt configuration\n",
        "    prompt_config = PROMPTS.get(prompt_type, PROMPTS[\"base_retrieval_prompt\"])\n",
        "    \n",
        "    # Format retrieved documents\n",
        "    docs_text = \"\"\n",
        "    for i, doc in enumerate(retrieved_docs[:3], 1):\n",
        "        content = doc['content'][:300] + \"...\" if len(doc['content']) > 300 else doc['content']\n",
        "        docs_text += f\"\\n[Document {i}: {doc['title']}]\\n{content}\\n\"\n",
        "    \n",
        "    # Build structured prompt\n",
        "    prompt = f\"{prompt_config['role']}\\n\\n\"\n",
        "    prompt += f\"Goal: {prompt_config['goal']}\\n\\n\"\n",
        "    prompt += \"Guidelines:\\n\"\n",
        "    for guideline in prompt_config['context_guidelines']:\n",
        "        prompt += f\"- {guideline}\\n\"\n",
        "    prompt += f\"\\nContext:{docs_text}\\n\"\n",
        "    prompt += f\"Question: {query}\\n\\n\"\n",
        "    prompt += f\"{prompt_config['response_format']}\\n\\nAnswer:\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def generate_response(query: str, top_k: int = 3, debug: bool = False):\n",
        "    \"\"\"Generate response using RAG pipeline.\"\"\"\n",
        "    \n",
        "    # Retrieve relevant documents\n",
        "    retrieved = retrieve_docs(query, top_k=top_k)\n",
        "    \n",
        "    if not retrieved:\n",
        "        return {\n",
        "            \"answer\": \"I couldn't find relevant information in the Shoplite knowledge base. Please try rephrasing your question.\",\n",
        "            \"sources\": [],\n",
        "            \"confidence\": \"Low\"\n",
        "        }\n",
        "    \n",
        "    top_score = max(d['score'] for d in retrieved)\n",
        "    \n",
        "    # Determine confidence and prompt type\n",
        "    if top_score < SIMILARITY_THRESHOLD:\n",
        "        return {\n",
        "            \"answer\": \"I don't have specific information about that in my knowledge base. I can help with Shoplite's registration, orders, payments, returns, seller accounts, and customer support features.\",\n",
        "            \"sources\": [],\n",
        "            \"confidence\": \"Low\"\n",
        "        }\n",
        "    \n",
        "    # Set confidence level\n",
        "    if top_score >= 0.65:\n",
        "        confidence = \"High\"\n",
        "        prompt_type = \"base_retrieval_prompt\"\n",
        "    elif top_score >= 0.45:\n",
        "        confidence = \"Medium\"\n",
        "        prompt_type = \"base_retrieval_prompt\"\n",
        "    else:\n",
        "        confidence = \"Low\"\n",
        "        prompt_type = \"clarification_prompt\"\n",
        "    \n",
        "    # Build prompt with structured format\n",
        "    prompt = build_prompt_from_retrieval(query, retrieved, prompt_type)\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"\\n[DEBUG] Top score: {top_score:.3f}, Confidence: {confidence}\")\n",
        "        print(f\"[DEBUG] Prompt length: {len(prompt)} chars\")\n",
        "    \n",
        "    # If no model, use extractive approach\n",
        "    if model is None:\n",
        "        sentences = []\n",
        "        for doc in retrieved[:2]:\n",
        "            doc_sentences = [s.strip() + '.' for s in doc['content'].split('.') if len(s.strip()) > 30][:2]\n",
        "            sentences.extend(doc_sentences)\n",
        "        \n",
        "        answer = ' '.join(sentences[:3])\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sources\": [d['title'] for d in retrieved[:2]],\n",
        "            \"confidence\": confidence\n",
        "        }\n",
        "    \n",
        "    # Generate with LLM\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1800)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                temperature=TEMPERATURE,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.2,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Extract only the generated text\n",
        "        generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "        raw_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "        \n",
        "        if debug:\n",
        "            print(f\"[DEBUG] Raw output: {raw_text[:150]}...\")\n",
        "        \n",
        "        # Clean up the response - extract first 2-3 sentences\n",
        "        sentences = []\n",
        "        current = \"\"\n",
        "        \n",
        "        for char in raw_text:\n",
        "            current += char\n",
        "            if char in '.!?' and len(current.strip()) > 20:\n",
        "                sentences.append(current.strip())\n",
        "                current = \"\"\n",
        "                if len(sentences) >= 3:\n",
        "                    break\n",
        "        \n",
        "        answer = ' '.join(sentences[:3]) if sentences else raw_text[:300]\n",
        "        \n",
        "        # Remove incomplete final sentence\n",
        "        if answer and not any(answer.endswith(p) for p in ['.', '!', '?']):\n",
        "            last_period = max(answer.rfind('.'), answer.rfind('!'), answer.rfind('?'))\n",
        "            if last_period > 0:\n",
        "                answer = answer[:last_period + 1]\n",
        "        \n",
        "        # Fallback to extractive if generation too short\n",
        "        if len(answer) < 30:\n",
        "            doc = retrieved[0]\n",
        "            sentences = [s.strip() + '.' for s in doc['content'].split('.') if len(s.strip()) > 25][:2]\n",
        "            answer = ' '.join(sentences)\n",
        "        \n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sources\": [d['title'] for d in retrieved[:3]],\n",
        "            \"confidence\": confidence\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "            print(f\"[DEBUG] Generation error: {e}\")\n",
        "        \n",
        "        # Fallback to extractive\n",
        "        doc = retrieved[0]\n",
        "        sentences = [s.strip() + '.' for s in doc['content'].split('.') if len(s.strip()) > 25][:2]\n",
        "        return {\n",
        "            \"answer\": ' '.join(sentences),\n",
        "            \"sources\": [d['title'] for d in retrieved[:2]],\n",
        "            \"confidence\": \"Medium\"\n",
        "        }\n",
        "\n",
        "print(\"✅ Generation pipeline configured\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flask_api_setup"
      },
      "source": [
        "# Cell 9: Flask API Setup\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    \"\"\"Health check endpoint.\"\"\"\n",
        "    return jsonify({\n",
        "        \"status\": \"running\",\n",
        "        \"model_loaded\": model is not None,\n",
        "        \"model_name\": loaded_model_name if model else \"None\",\n",
        "        \"num_docs\": len(KNOWLEDGE_BASE),\n",
        "        \"embedding_model\": EMBED_MODEL_NAME\n",
        "    })\n",
        "\n",
        "@app.route('/ping', methods=['POST'])\n",
        "def ping():\n",
        "    \"\"\"Direct LLM interaction without RAG.\"\"\"\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        query = data.get('query', '')\n",
        "        \n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"No query provided\"}), 400\n",
        "        \n",
        "        if model is None:\n",
        "            return jsonify({\"error\": \"Model not loaded\"}), 503\n",
        "        \n",
        "        inputs = tokenizer(query, return_tensors='pt', truncation=True, max_length=512)\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "        \n",
        "        return jsonify({\n",
        "            \"response\": response.strip(),\n",
        "            \"query\": query\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    \"\"\"RAG-powered chat endpoint.\"\"\"\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        query = data.get('query', '')\n",
        "        top_k = data.get('top_k', 3)\n",
        "        \n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"No query provided\"}), 400\n",
        "        \n",
        "        result = generate_response(query, top_k=top_k)\n",
        "        \n",
        "        return jsonify(result)\n",
        "        \n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "print(\"✅ Flask API configured with /health, /ping, /chat endpoints\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngrok_tunnel_setup"
      },
      "source": [
        "# Cell 10: ngrok Tunnel Setup\n",
        "print(\"🔐 ngrok Setup\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# SECURITY: Accept token at runtime\n",
        "ngrok_token = input(\"Enter your ngrok auth token: \").strip()\n",
        "\n",
        "if not ngrok_token:\n",
        "    print(\"❌ No token provided. Cannot create tunnel.\")\n",
        "else:\n",
        "    try:\n",
        "        # Set ngrok token\n",
        "        ngrok.set_auth_token(ngrok_token)\n",
        "        \n",
        "        # Start Flask in background thread\n",
        "        def run_flask():\n",
        "            app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n",
        "        \n",
        "        flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
        "        flask_thread.start()\n",
        "        \n",
        "        # Wait for Flask to start\n",
        "        time.sleep(3)\n",
        "        \n",
        "        # Create ngrok tunnel\n",
        "        public_url = ngrok.connect(5000, bind_tls=True)\n",
        "        \n",
        "        print(\"\\n✅ TUNNEL ACTIVE\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"🌐 Public URL: {public_url}\")\n",
        "        print(\"=\" * 50)\n",
        "        print(\"\\nAPI Endpoints:\")\n",
        "        print(f\"  • Health: {public_url}/health\")\n",
        "        print(f\"  • Chat:   {public_url}/chat\")\n",
        "        print(f\"  • Ping:   {public_url}/ping\")\n",
        "        print(\"\\n💡 Copy the URL above to use in chat-interface.py\")\n",
        "        print(\"⚠️  Keep this cell running - do not stop execution!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Tunnel creation failed: {e}\")\n",
        "        print(\"Please check your ngrok token and try again.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "system_validation"
      },
      "source": [
        "# Cell 11: System Validation and Testing\n",
        "print(\"🧪 Running System Tests...\\n\")\n",
        "\n",
        "# Test 1: Retrieval Quality\n",
        "print(\"Test 1: Document Retrieval\")\n",
        "test_q = \"How do I create a seller account?\"\n",
        "docs = retrieve_docs(test_q, top_k=3)\n",
        "print(f\"✅ Retrieved {len(docs)} docs\")\n",
        "print(f\"   Top: {docs[0]['title']} (score: {docs[0]['score']:.3f})\")\n",
        "print(f\"   Expected: 'Seller Account Setup and Management'\")\n",
        "assert \"Seller\" in docs[0]['title'], \"Top document should be about seller accounts\"\n",
        "\n",
        "# Test 2: Response Generation\n",
        "print(\"\\nTest 2: Response Generation\")\n",
        "response = generate_response(test_q)\n",
        "print(f\"✅ Generated response ({len(response['answer'])} chars)\")\n",
        "print(f\"   Answer preview: {response['answer'][:100]}...\")\n",
        "print(f\"   Sources: {', '.join(response['sources'][:2])}\")\n",
        "print(f\"   Confidence: {response['confidence']}\")\n",
        "assert len(response['answer']) > 20, \"Response should be substantial\"\n",
        "assert response['confidence'] in ['High', 'Medium', 'Low'], \"Invalid confidence level\"\n",
        "\n",
        "# Test 3: Multi-document Query\n",
        "print(\"\\nTest 3: Multi-Document Query\")\n",
        "multi_q = \"What are the return policies and how do I track my order?\"\n",
        "response = generate_response(multi_q)\n",
        "print(f\"✅ Synthesized from {len(response['sources'])} sources\")\n",
        "print(f\"   Sources: {', '.join(response['sources'])}\")\n",
        "assert len(response['sources']) >= 2, \"Should retrieve multiple relevant documents\"\n",
        "\n",
        "# Test 4: Edge Case - Out of Scope\n",
        "print(\"\\nTest 4: Out-of-Scope Query\")\n",
        "oos_q = \"What's the weather like today?\"\n",
        "response = generate_response(oos_q)\n",
        "print(f\"✅ Handled correctly: '{response['answer'][:80]}...'\")\n",
        "print(f\"   Confidence: {response['confidence']}\")\n",
        "assert response['confidence'] == 'Low', \"Out-of-scope should have low confidence\"\n",
        "\n",
        "# Test 5: Low Similarity Query\n",
        "print(\"\\nTest 5: Ambiguous Query\")\n",
        "amb_q = \"Tell me about it\"\n",
        "response = generate_response(amb_q)\n",
        "print(f\"✅ Response: '{response['answer'][:80]}...'\")\n",
        "print(f\"   Confidence: {response['confidence']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"✅ ALL TESTS PASSED - System Ready for Deployment!\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\n📋 System Summary:\")\n",
        "print(f\"  • Knowledge Base: {len(KNOWLEDGE_BASE)} documents\")\n",
        "print(f\"  • Embedding Model: {EMBED_MODEL_NAME}\")\n",
        "print(f\"  • LLM: {loaded_model_name if model else 'Extractive Mode'}\")\n",
        "print(f\"  • FAISS Index: {index.ntotal} vectors\")\n",
        "print(f\"  • Prompt Templates: {len(PROMPTS) - 3} configured\")\n",
        "print(\"\\n🚀 Ready to accept chat requests!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
