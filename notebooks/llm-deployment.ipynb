{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Deployment for Shoplite (Colab-ready)
",
        "
",
        "This notebook is self-contained. It embeds the Shoplite knowledge base, builds a FAISS index with `sentence-transformers`, loads an open-source LLM (Llama 3.1 8B by default), exposes a Flask API (`/chat`, `/ping`, `/health`) and uses `pyngrok` to create a public tunnel.
",
        "
",
        "**IMPORTANT**: Instructors must supply their ngrok authtoken via an `input()` prompt at runtime. Do NOT hardcode tokens. If the model cannot be loaded in Colab due to size or access restrictions, follow the troubleshooting notes in the last cell to switch to a smaller model or use quantized loading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Install dependencies
",
        "# Run this cell in Colab (ensure GPU runtime selected: Runtime -> Change runtime type -> GPU)
",
        "!pip install --quiet --upgrade pip
",
        "!pip install --quiet transformers accelerate bitsandbytes safetensors sentence-transformers faiss-cpu flask pyngrok uvicorn gunicorn -U
",
        "# bitsandbytes is used for 8-bit loading/quantization; accelerate helps with device mapping
"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Imports and utilities
",
        "import os, time, json, threading
",
        "from typing import List, Dict, Any
",
        "from flask import Flask, request, jsonify
",
        "import numpy as np
",
        "import faiss
",
        "from sentence_transformers import SentenceTransformer
",
        "import torch
",
        "from transformers import AutoTokenizer, AutoModelForCausalLM
",
        "from pyngrok import ngrok
",
        "
",
        "# Helper to safely serialize numpy arrays to JSON
",
        "class NpEncoder(json.JSONEncoder):
",
        "    def default(self, obj):
",
        "        if isinstance(obj, np.ndarray):
",
        "            return obj.tolist()
",
        "        return super().default(obj)
"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Knowledge Base (embedded)
",
        "KNOWLEDGE_BASE = [
",
        "    {\"id\": \"doc1\", \"title\": \"Shoplite User Registration and Account Management\", \"content\": \"To create a Shoplite account, users must visit the registration page and provide a valid email address, password, and basic profile information. Email verification is required within 24 hours. Users can choose between: - Buyer accounts (free) - Seller accounts (requires business verification and tax information). Account Management Features: Update personal information, Change passwords, Set security questions, Manage notification preferences, Deactivate accounts (requires email confirmation; may affect active orders/subscriptions). Buyer Access: product browsing, purchasing, order tracking, reviews. Seller Access: seller dashboard, inventory management, order processing, analytics. Security Measures: two-factor authentication recommended, password recovery via email and phone verification.\"},
",
        "    {\"id\": \"doc2\", \"title\": \"Shoplite Product Search and Filtering Features\", \"content\": \"Shoplite provides a powerful search engine: Search Capabilities: keyword queries, category selection, brand filters. Filtering Options: price range, rating, availability, seller location, shipping speed, promotions, eco-friendly options. Features: Autocomplete suggestions, Spelling correction, Save searches & alerts, Faceted navigation, Optimized for large catalogs with real-time indexing, Mobile responsive interface.\"},
",
        "    {\"id\": \"doc3\", \"title\": \"Shoplite Shopping Cart and Checkout Process\", \"content\": \"Add multiple items from different sellers; Review quantities, apply promo codes/gift cards; Cart preserved across sessions for logged-in users. Checkout Steps: 1. Shipping selection (standard, expedited, same-day) 2. Payment selection (credit/debit cards, digital wallets, cash-on-delivery) 3. Order confirmation. Security & Processing: PCI-DSS compliant payment gateways, Real-time stock updates, Order confirmation emails with tracking, Seller notifications for new orders, Integrated returns and refunds system.\"},
",
        "    {\"id\": \"doc4\", \"title\": \"Shoplite Payment Methods and Security\", \"content\": \"Accepted Payment Methods: credit/debit cards, PayPal, Apple Pay, Google Pay, local solutions. Security Measures: SSL encryption, PCI-DSS compliance, fraud detection, two-factor authentication, sensitive info encrypted in transit and at rest. Other Features: digital wallet integration, structured dispute/chargeback process, seller payments after order confirmation.\"},
",
        "    {\"id\": \"doc5\", \"title\": \"Shoplite Order Tracking and Delivery\", \"content\": \"Real-time tracking with confirmation emails and unique tracking number. Stages: confirmed -> processing -> shipped -> in transit -> delivered. Delivery modification requests (seller approval required). International shipments display customs/import duties. Optimized logistics with estimated arrival and delay notifications. Support assistance for lost/delayed packages.\"},
",
        "    {\"id\": \"doc6\", \"title\": \"Shoplite Return and Refund Policies\", \"content\": \"Return Period: typically 30 days from delivery. Process: select order/item, specify reason, use prepaid label if eligible. Refunds: processed in 5â€“7 business days to original payment method. Digital/personalized items may have exceptions. Automated order status updates. Sellers must comply with policies to maintain ratings. Dispute resolution available.\"},
",
        "    {\"id\": \"doc7\", \"title\": \"Shoplite Product Reviews and Ratings\", \"content\": \"Buyers rate products on a five-star scale and leave comments. Reviews moderated for compliance. Sellers can respond to reviews. Ratings influence search ranking. Verified purchase badges for authenticity. Aggregate ratings provided. Review analytics available for sellers.\"},
",
        "    {\"id\": \"doc8\", \"title\": \"Shoplite Seller Account Setup and Management\", \"content\": \"Create seller account with business documents and tax verification. Seller Dashboard: inventory management, order processing, sales analytics. Product listing via individual or bulk upload (CSV/API). Profile customization: branding, policies, shipping, returns. Notifications: new orders, low stock, inquiries. Pricing, promotions, and shipping fee management. Performance metrics tracked; third-party integrations supported.\"},
",
        "    {\"id\": \"doc9\", \"title\": \"Shoplite Inventory Management for Sellers\", \"content\": \"Track stock levels, reorder thresholds, and availability in real-time. Low-stock alerts. Bulk imports supported. Variants (size, color, bundle) supported. Inventory reports for trends and seasonal demand. Manage warehouses and shipping locations.\"},
",
        "    {\"id\": \"doc10\", \"title\": \"Shoplite Commission and Fee Structure\", \"content\": \"Commission fees per product category. Additional fees: premium listings, promotions, special services. Transparent notifications in dashboard. Payments made after commission deduction (weekly/bi-weekly). Transaction reports available. Pricing guidance provided.\"},
",
        "    {\"id\": \"doc11\", \"title\": \"Shoplite Customer Support Procedures\", \"content\": \"Support via live chat, email, phone, and AI chatbot (24/7). Ticket categorization: orders, payments, returns, technical, account management. Unique tracking IDs. Backend integration for order/payment info. Dedicated seller support channel. Help center with guides, FAQs, videos. Fast, transparent, fair resolution.\"},
",
        "    {\"id\": \"doc12\", \"title\": \"Shoplite Mobile App Features\", \"content\": \"iOS & Android support. Browse, filter, add to cart, purchase. Push notifications for promotions/order updates. Barcode scanning and QR code payments. Mobile wallets, fingerprint, Face ID login. Seller management on-the-go. Offline caching for previously loaded content. Intuitive, responsive, accessible interface.\"},
",
        "    {\"id\": \"doc13\", \"title\": \"Shoplite API Documentation for Developers\", \"content\": \"RESTful API endpoints: product catalog, orders, accounts, inventory. OAuth 2.0 authentication. Rate limiting (higher for verified partners). Detailed docs: request/response, parameters, error codes. Webhooks for real-time events. Sandbox environment for testing. Versioned API with backward-compatible updates.\"},
",
        "    {\"id\": \"doc14\", \"title\": \"Shoplite Security and Privacy Policies\", \"content\": \"Data Protection: TLS encryption, AES-256 at rest, authorized access. Two-factor authentication & strong passwords. GDPR & CCPA compliance. Security monitoring for suspicious activity. Clear privacy policies: data collection, usage, third-party sharing. Policy change notifications; user control over data.\"},
",
        "    {\"id\": \"doc15\", \"title\": \"Shoplite Promotional Codes and Discounts\", \"content\": \"Sellers create promotions: discount codes, seasonal sales, bundle offers. Code types: percentage, fixed, conditional. Start/end dates, usage limits, minimum purchase configurable. Automatic verification at checkout. Analytics: redemption, revenue, engagement. User notifications for active promotions. Special events highlighted on homepage/app. Compliance with platform policies.\"}
",
        "]
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Prompts embedded as Python dict (converted from assistant-prompts.yml)
",
        "PROMPTS = {
",
        "    \"version\": \"1.0\",
",
        "    \"created\": \"2025-09-23\",
",
        "    \"author\": \"Joseph Chamoun\",
",
        "    \"base_retrieval_prompt\": {
",
        "        \"role\": \"You are a helpful Shoplite customer service assistant.\",
",
        "        \"goal\": \"Provide accurate answers using only the provided Shoplite documentation.\",
",
        "        \"context_guidelines\": [\"Use only information from the provided document snippets\", \"Cite specific documents when possible\"],
",
        "        \"response_format\": \"Answer: [Your response based on context]\nSources: [List document titles referenced]\"
",
        "    },
",
        "    \"multi_doc_synthesis\": {
",
        "        \"role\": \"You are an expert Shoplite support agent who synthesizes multiple documents.\",
",
        "        \"goal\": \"Combine information from multiple retrieved documents to create a concise, accurate answer.\",
",
        "        \"context_guidelines\": [\"State which documents you used\", \"When information conflicts, show both options and recommend the safer/default one\"],
",
        "        \"response_format\": \"Answer: [Synthesis]\nSources: [Doc titles]\nConfidence: [High|Medium|Low]\"
",
        "    },
",
        "    \"refusal_when_no_context\": {
",
        "        \"role\": \"You are a safety-conscious assistant.\",
",
        "        \"goal\": \"Refuse to answer if no relevant context is found in the knowledge base and ask for clarification or external data.\",
",
        "        \"context_guidelines\": [\"If top retrieved documents have low similarity (< threshold), return a refusal\", \"Suggest the user provide more details or check the Shoplite help center\"],
",
        "        \"response_format\": \"Answer: I don\'t have enough information in the Shoplite docs to answer that. Please provide more details or check [Help Center].\"
",
        "    }
",
        "}
"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Build embeddings and FAISS index
",
        "EMBED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
",
        "print('Loading embedding model:', EMBED_MODEL_NAME)
",
        "embed_model = SentenceTransformer(EMBED_MODEL_NAME)
",
        "
",
        "DOCUMENT_TEXTS = [d['title'] + '\n\n' + d['content'] for d in KNOWLEDGE_BASE]
",
        "DOC_IDS = [d['id'] for d in KNOWLEDGE_BASE]
",
        "
",
        "print('Encoding documents...')
",
        "doc_embeddings = embed_model.encode(DOCUMENT_TEXTS, convert_to_numpy=True, show_progress_bar=True)
",
        "
",
        "def normalize_embeddings(embs):
",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True)
",
        "    return embs / np.clip(norms, a_min=1e-10, a_max=None)
",
        "
",
        "doc_embeddings = normalize_embeddings(doc_embeddings)
",
        "d = doc_embeddings.shape[1]
",
        "index = faiss.IndexFlatIP(d)
",
        "index.add(doc_embeddings)
",
        "print(f'FAISS index created with {index.ntotal} vectors (dim={d})')
"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 6: Retrieval functions
",
        "def retrieve_docs(query: str, top_k: int = 3):
",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True)
",
        "    q_emb = normalize_embeddings(q_emb)
",
        "    scores, indices = index.search(q_emb, top_k)
",
        "    results = []
",
        "    for score, idx in zip(scores[0], indices[0]):
",
        "        if idx < 0 or idx >= len(KNOWLEDGE_BASE):
",
        "            continue
",
        "        doc = KNOWLEDGE_BASE[idx]
",
        "        results.append({
",
        "            'id': doc['id'],
",
        "            'title': doc['title'],
",
        "            'content': doc['content'],
",
        "            'score': float(score)
",
        "        })
",
        "    return results
",
        "
",
        "# Quick test (optional)
",
        "print(retrieve_docs('How do I create a seller account on Shoplite?', top_k=3))
"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 7: Model loading (Llama 3.1 8B suggested) with quantization guidance
",
        "MODEL_NAME = 'meta-llama/Llama-3.1-8B-Instruct'
",
        "print('Attempting to load model:', MODEL_NAME)
",
        "use_cuda = torch.cuda.is_available()
",
        "print('CUDA available:', use_cuda)
",
        "
",
        "# Recommended: use bitsandbytes + accelerate for 8-bit loading in Colab
",
        "try:
",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
",
        "    # Use 8-bit loading if possible
",
        "    from transformers import AutoModelForCausalLM
",
        "    from accelerate import init_empty_weights, load_checkpoint_and_dispatch
",
        "    # Conservative attempt: try standard load, otherwise fall back to placeholder
",
        "    try:
",
        "        model = AutoModelForCausalLM.from_pretrained(
",
        "            MODEL_NAME,
",
        "            torch_dtype=torch.float16 if use_cuda else torch.float32,
",
        "            device_map='auto' if use_cuda else None,
",
        "            low_cpu_mem_usage=True,
",
        "        )
",
        "        model.eval()
",
        "        print('Model loaded successfully (standard load).')
",
        "    except Exception as e_inner:
",
        "        print('Standard load failed, attempting 8-bit load with bitsandbytes/accelerate:', e_inner)
",
        "        try:
",
        "            # 8-bit loading example (may require additional configuration on Colab)
",
        "            model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, load_in_8bit=True, device_map='auto')
",
        "            model.eval()
",
        "            print('Model loaded with 8-bit quantization (bitsandbytes).')
",
        "        except Exception as e_bnb:
",
        "            print('8-bit load failed:', e_bnb)
",
        "            model = None
",
        "except Exception as e:
",
        "    print('Model load configuration failed. Exception:', e)
",
        "    model = None
"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 8: Prompt building + generation pipeline
",
        "TEMPERATURE = 0.0
",
        "MAX_TOKENS = 512
",
        "
",
        "def build_prompt_from_retrieval(query: str, retrieved_docs: List[Dict[str, Any]], prompt_template: Dict[str, Any] = None):
",
        "    if prompt_template is None:
",
        "        prompt_template = PROMPTS['base_retrieval_prompt']
",
        "    role = prompt_template['role']
",
        "    goal = prompt_template['goal']
",
        "    context_guidelines = '\n'.join(f'- {l}' for l in prompt_template.get('context_guidelines', []))
",
        "    response_format = prompt_template.get('response_format', '')
",
        "    docs_text = '\n\n---\n\n'.join([f\"Title: {d['title']}\nContent:\n{d['content']}\" for d in retrieved_docs])
",
        "    prompt = f\"{role}\nGoal: {goal}\n\nContext Guidelines:\n{context_guidelines}\n\nRetrieved Documents:\n{docs_text}\n\nUser Query: {query}\n\n{response_format}\n\"
",
        "    return prompt
",
        "
",
        "def generate_response(query: str, top_k: int = 3, temperature: float = TEMPERATURE, max_tokens: int = MAX_TOKENS):
",
        "    retrieved = retrieve_docs(query, top_k=top_k)
",
        "    prompt = build_prompt_from_retrieval(query, retrieved, PROMPTS.get('multi_doc_synthesis'))
",
        "    if model is None:
",
        "        # Fallback: return retrieved summaries when model not available
",
        "        answer = 'LLM not loaded in this environment. Top retrieved docs:\n'
",
        "        for d in retrieved:
",
        "            answer += f\"- {d['title']} (score={d['score']:.3f})\n\"
",
        "        return {'answer': answer, 'sources': [d['title'] for d in retrieved], 'confidence': 'Low'}
",
        "
",
        "    # Tokenize and run model
",
        "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048)
",
        "    if torch.cuda.is_available():
",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}
",
        "        model.to('cuda')
",
        "    with torch.no_grad():
",
        "        outputs = model.generate(**inputs, do_sample=(temperature>0), temperature=temperature, max_new_tokens=max_tokens, top_p=0.95, eos_token_id=tokenizer.eos_token_id)
",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
",
        "    if prompt in text:
",
        "        text = text.split(prompt)[-1].strip()
",
        "    return {'answer': text, 'sources': [d['title'] for d in retrieved], 'confidence': 'Medium'}
"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 9: Flask API (chat, ping, health)
",
        "app = Flask(__name__)
",
        "
",
        "@app.route('/health', methods=['GET'])
",
        "def health():
",
        "    return jsonify({'status': 'ok', 'model_loaded': model is not None, 'num_docs': len(KNOWLEDGE_BASE)})
",
        "
",
        "@app.route('/ping', methods=['POST'])
",
        "def ping():
",
        "    data = request.json or {}
",
        "    text = data.get('text', 'Hello')
",
        "    if model is None:
",
        "        return jsonify({'reply': f'Model not loaded. Echo: {text}'})
",
        "    prompt = f\"{PROMPTS['base_retrieval_prompt']['role']}\nUser: {text}\nRespond briefly.\"
",
        "    inputs = tokenizer(prompt, return_tensors='pt')
",
        "    if torch.cuda.is_available():
",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}
",
        "        model.to('cuda')
",
        "    with torch.no_grad():
",
        "        outputs = model.generate(**inputs, max_new_tokens=128)
",
        "    reply = tokenizer.decode(outputs[0], skip_special_tokens=True)
",
        "    if prompt in reply:
",
        "        reply = reply.split(prompt)[-1].strip()
",
        "    return jsonify({'reply': reply})
",
        "
",
        "@app.route('/chat', methods=['POST'])
",
        "def chat():
",
        "    data = request.json or {}
",
        "    query = data.get('query')
",
        "    top_k = int(data.get('top_k', 3))
",
        "    if not query:
",
        "        return jsonify({'error': 'missing query'}), 400
",
        "    try:
",
        "        result = generate_response(query, top_k=top_k)
",
        "        return jsonify(result)
",
        "    except Exception as e:
",
        "        return jsonify({'error': str(e)}), 500
",
        "
",
        "# Run Flask in background thread so notebook remains interactive
",
        "def run_flask():
",
        "    app.run(host='0.0.0.0', port=5000)
",
        "
",
        "thread = threading.Thread(target=run_flask, daemon=True)
",
        "thread.start()
",
        "print('Flask server started (background thread).')
"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 10: ngrok tunnel setup (prompt for authtoken)
",
        "print('
=== NGROK TUNNEL SETUP ===')
",
        "ngrok_token = input('Enter your ngrok authtoken (paste it here): ').strip()
",
        "if ngrok_token:
",
        "    try:
",
        "        ngrok.set_auth_token(ngrok_token)
",
        "        public_url = ngrok.connect(5000)
",
        "        print('ngrok tunnel created at:', public_url)
",
        "    except Exception as e:
",
        "        print('Failed to create ngrok tunnel:', e)
",
        "        public_url = None
",
        "else:
",
        "    print('No ngrok token provided; remember to set up a tunnel separately.')
",
        "    public_url = None
",
        "
",
        "print('If public_url is not None, use it to call the /chat endpoint from outside Colab.')
"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 11: Quick test examples (run after ngrok created and model loaded)
",
        "print('Local health check:')
",
        "try:
",
        "    import requests
",
        "    r = requests.get('http://127.0.0.1:5000/health', timeout=5)
",
        "    print('Health:', r.json())
",
        "except Exception as e:
",
        "    print('Local health check failed:', e)
",
        "
",
        "if 'public_url' in globals() and public_url:
",
        "    print(f'Call this externally: POST {public_url}/chat with JSON {\\"query\\": \\"How do I create a seller account on Shoplite?\\"}')
"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notes & Troubleshooting
",
        "
",
        "- **Model load failures**: Llama 3.1 8B is large and may not load on free Colab GPUs or without HF access. If loading fails, switch `MODEL_NAME` to a smaller model or use `load_in_8bit=True` with bitsandbytes.
",
        "- **Quantization**: Use `bitsandbytes` + `accelerate` for 8-bit loading. Some setups require configuring `~/.cache/huggingface` with tokens/credentials.
",
        "- **Security**: Never commit your ngrok authtoken. Enter it at runtime when prompted.
",
        "- **Persistence**: Colab runtimes are ephemeral; save `doc_embeddings.npy` and `kb.json` if you want to reuse them within the same session.
",
        "
",
        "If you want, I can now generate the `/src/chat-interface.py` CLI script that connects to the ngrok endpoint and demonstrates example queries." 
      ]
    }
  ]
}
