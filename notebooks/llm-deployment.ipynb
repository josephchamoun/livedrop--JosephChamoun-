# %% [markdown]
# LLM Deployment Colab Notebook

This notebook is a **self-contained** Colab-ready script to deploy a RAG API for the Shoplite knowledge base you provided.

**Features:**
- Embeds the Shoplite knowledge base directly as Python data structures
- Builds FAISS index using sentence-transformers embeddings
- Loads an open-source LLM (example: Llama 3.1 8B or a compatible model) with optional quantization for Colab GPUs
- Implements Retrieval-Augmented Generation (RAG)
- Exposes a Flask API with `/chat`, `/ping`, `/health`
- Uses `pyngrok` to open a tunnel; prompts for ngrok token via `input()` (no hardcoded tokens)

**How to use in Colab:**
1. Create a new Colab notebook and paste the contents of each code cell below into separate Colab cells in the same order.
2. Run the install cell (Cell 1).
3. Enter your ngrok token when prompted at the ngrok cell.
4. Test the `/chat` endpoint from your local CLI or using the provided test cell.

---

# %%
# Cell 1: Install dependencies
# NOTE: On Colab you may need to select a GPU runtime (Runtime > Change runtime type > GPU).
!pip install --quiet --upgrade pip
!pip install --quiet transformers accelerate einops safetensors sentence-transformers faiss-cpu flask pyngrok uvicorn gunicorn

# The versions above are examples. If you plan to run a specific LLM provider (llama.cpp, ggml, etc.) add the installation steps here.

# %%
# Cell 2: Imports and basic utilities
import os
import time
import json
import threading
from typing import List, Dict, Any

from flask import Flask, request, jsonify

# Embedding / FAISS
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

# LLM (transformers)
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ngrok
from pyngrok import ngrok

# Helper: safe JSON serializer for numpy
class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)

# %%
# Cell 3: Knowledge base (embedded directly)
KNOWLEDGE_BASE = [
    {"id": "doc1", "title": "Shoplite User Registration and Account Management", "content": """
To create a Shoplite account, users must visit the registration page and provide a valid email address, password, and basic profile information. Email verification is required within 24 hours. Users can choose between:\n\n- Buyer accounts (free)\n- Seller accounts (requires business verification and tax information)\n\nAccount Management Features:\n- Update personal information\n- Change passwords\n- Set security questions\n- Manage notification preferences\n- Deactivate accounts (requires email confirmation; may affect active orders/subscriptions)\n\nBuyer Access: product browsing, purchasing, order tracking, reviews\nSeller Access: seller dashboard, inventory management, order processing, analytics\n\nSecurity Measures: two-factor authentication recommended, password recovery via email and phone verification.
"""},

    {"id": "doc2", "title": "Shoplite Product Search and Filtering Features", "content": """
Shoplite provides a powerful search engine:\n\n- Search Capabilities: keyword queries, category selection, brand filters\n- Filtering Options: price range, rating, availability, seller location, shipping speed, promotions, eco-friendly options\n\nFeatures:\n- Autocomplete suggestions\n- Spelling correction\n- Save searches & alerts\n- Faceted navigation for combining multiple filters\n- Optimized for large catalogs with real-time indexing\n- Mobile responsive interface
"""},

    {"id": "doc3", "title": "Shoplite Shopping Cart and Checkout Process", "content": """
- Add multiple items from different sellers\n- Review quantities, apply promo codes/gift cards\n- Cart preserved across sessions for logged-in users\n\nCheckout Steps:\n1. Shipping selection (standard, expedited, same-day)\n2. Payment selection (credit/debit cards, digital wallets, cash-on-delivery)\n3. Order confirmation\n\nSecurity & Processing:\n- PCI-DSS compliant payment gateways\n- Real-time stock updates\n- Order confirmation emails with tracking\n- Seller notifications for new orders\n- Integrated returns and refunds system
"""},

    {"id": "doc4", "title": "Shoplite Payment Methods and Security", "content": """
- Accepted Payment Methods: credit/debit cards, PayPal, Apple Pay, Google Pay, local solutions\n- Security Measures: SSL encryption, PCI-DSS compliance, fraud detection, two-factor authentication, sensitive info encrypted in transit and at rest\n- Other Features: digital wallet integration, structured dispute/chargeback process, seller payments after order confirmation
"""},

    {"id": "doc5", "title": "Shoplite Order Tracking and Delivery", "content": """
- Real-time tracking with confirmation emails and unique tracking number\n\nStages: confirmed -> processing -> shipped -> in transit -> delivered\n\n- Delivery modification requests (seller approval required)\n- International shipments display customs/import duties\n- Optimized logistics with estimated arrival and delay notifications\n- Support assistance for lost/delayed packages
"""},

    {"id": "doc6", "title": "Shoplite Return and Refund Policies", "content": """
- Return Period: typically 30 days from delivery\n- Process: select order/item, specify reason, use prepaid label if eligible\n- Refunds: processed in 5â€“7 business days to original payment method\n- Digital/personalized items may have exceptions\n- Automated order status updates\n- Sellers must comply with policies to maintain ratings\n- Dispute resolution available
"""},

    {"id": "doc7", "title": "Shoplite Product Reviews and Ratings", "content": """
- Buyers rate products on a five-star scale and leave comments\n- Reviews moderated for compliance\n- Sellers can respond to reviews\n- Ratings influence search ranking\n- Verified purchase badges for authenticity\n- Aggregate ratings provided\n- Review analytics available for sellers
"""},

    {"id": "doc8", "title": "Shoplite Seller Account Setup and Management", "content": """
- Create seller account with business documents and tax verification\n- Seller Dashboard: inventory management, order processing, sales analytics\n- Product listing via individual or bulk upload (CSV/API)\n- Profile customization: branding, policies, shipping, returns\n- Notifications: new orders, low stock, inquiries\n- Pricing, promotions, and shipping fee management\n- Performance metrics tracked; third-party integrations supported
"""},

    {"id": "doc9", "title": "Shoplite Inventory Management for Sellers", "content": """
- Track stock levels, reorder thresholds, and availability in real-time\n- Low-stock alerts\n- Bulk imports supported\n- Variants (size, color, bundle) supported\n- Inventory reports for trends and seasonal demand\n- Manage warehouses and shipping locations
"""},

    {"id": "doc10", "title": "Shoplite Commission and Fee Structure", "content": """
- Commission fees per product category\n- Additional fees: premium listings, promotions, special services\n- Transparent notifications in dashboard\n- Payments made after commission deduction (weekly/bi-weekly)\n- Transaction reports available\n- Pricing guidance provided
"""},

    {"id": "doc11", "title": "Shoplite Customer Support Procedures", "content": """
- Support via live chat, email, phone, and AI chatbot (24/7)\n- Ticket categorization: orders, payments, returns, technical, account management\n- Unique tracking IDs\n- Backend integration for order/payment info\n- Dedicated seller support channel\n- Help center with guides, FAQs, videos\n- Fast, transparent, fair resolution
"""},

    {"id": "doc12", "title": "Shoplite Mobile App Features", "content": """
- iOS & Android support\n- Browse, filter, add to cart, purchase\n- Push notifications for promotions/order updates\n- Barcode scanning and QR code payments\n- Mobile wallets, fingerprint, Face ID login\n- Seller management on-the-go\n- Offline caching for previously loaded content\n- Intuitive, responsive, accessible interface
"""},

    {"id": "doc13", "title": "Shoplite API Documentation for Developers", "content": """
- RESTful API endpoints: product catalog, orders, accounts, inventory\n- OAuth 2.0 authentication\n- Rate limiting (higher for verified partners)\n- Detailed docs: request/response, parameters, error codes\n- Webhooks for real-time events\n- Sandbox environment for testing\n- Versioned API with backward-compatible updates
"""},

    {"id": "doc14", "title": "Shoplite Security and Privacy Policies", "content": """
- Data Protection: TLS encryption, AES-256 at rest, authorized access\n- Two-factor authentication & strong passwords\n- GDPR & CCPA compliance\n- Security monitoring for suspicious activity\n- Clear privacy policies: data collection, usage, third-party sharing\n- Policy change notifications; user control over data
"""},

    {"id": "doc15", "title": "Shoplite Promotional Codes and Discounts", "content": """
- Sellers create promotions: discount codes, seasonal sales, bundle offers\n- Code types: percentage, fixed, conditional\n- Start/end dates, usage limits, minimum purchase configurable\n- Automatic verification at checkout\n- Analytics: redemption, revenue, engagement\n- User notifications for active promotions\n- Special events highlighted on homepage/app\n- Compliance with platform policies
"""}
]

# %%
# Cell 4: Prompts (embedded YAML as Python dict)
PROMPTS = {
    "version": "1.0",
    "created": "2025-09-23",
    "author": "Joseph Chamoun",
    "base_retrieval_prompt": {
        "role": "You are a helpful Shoplite customer service assistant.",
        "goal": "Provide accurate answers using only the provided Shoplite documentation.",
        "context_guidelines": [
            "Use only information from the provided document snippets",
            "Cite specific documents when possible"
        ],
        "response_format": "Answer: [Your response based on context]\nSources: [List document titles referenced]"
    },
    "multi_doc_synthesis": {
        "role": "You are an expert Shoplite support agent who synthesizes multiple documents.",
        "goal": "Combine information from multiple retrieved documents to create a concise, accurate answer.",
        "context_guidelines": [
            "State which documents you used",
            "When information conflicts, show both options and recommend the safer/default one"
        ],
        "response_format": "Answer: [Synthesis]\nSources: [Doc titles]\nConfidence: [High|Medium|Low]"
    },
    "refusal_when_no_context": {
        "role": "You are a safety-conscious assistant.",
        "goal": "Refuse to answer if no relevant context is found in the knowledge base and ask for clarification or external data.",
        "context_guidelines": [
            "If top retrieved documents have low similarity (< threshold), return a refusal",
            "Suggest the user provide more details or check the Shoplite help center"
        ],
        "response_format": "Answer: I don\'t have enough information in the Shoplite docs to answer that. Please provide more details or check [Help Center]."
    }
}

# %%
# Cell 5: Embedding model and FAISS index creation
EMBED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'  # compact & fast; swap if you prefer
print('Loading embedding model:', EMBED_MODEL_NAME)
embed_model = SentenceTransformer(EMBED_MODEL_NAME)

# Build embeddings for all docs
DOCUMENT_TEXTS = [d['title'] + '\n\n' + d['content'] for d in KNOWLEDGE_BASE]
DOC_IDS = [d['id'] for d in KNOWLEDGE_BASE]

print('Encoding documents...')
doc_embeddings = embed_model.encode(DOCUMENT_TEXTS, convert_to_numpy=True, show_progress_bar=True)

# Normalize (important for cosine similarity using inner product)
def normalize_embeddings(embs: np.ndarray) -> np.ndarray:
    norms = np.linalg.norm(embs, axis=1, keepdims=True)
    return embs / np.clip(norms, a_min=1e-10, a_max=None)

doc_embeddings = normalize_embeddings(doc_embeddings)

# Build FAISS index (IndexFlatIP on normalized vectors => cosine similarity)
d = doc_embeddings.shape[1]
index = faiss.IndexFlatIP(d)
index.add(doc_embeddings)
print(f'FAISS index created with {index.ntotal} vectors (dim={d})')

# Map index positions to doc ids
# For small KB we can keep doc_embeddings in memory for retrieving content

# %%
# Cell 6: Retrieval functions

def retrieve_docs(query: str, top_k: int = 3):
    q_emb = embed_model.encode([query], convert_to_numpy=True)
    q_emb = normalize_embeddings(q_emb)
    scores, indices = index.search(q_emb, top_k)
    results = []
    for score, idx in zip(scores[0], indices[0]):
        if idx < 0 or idx >= len(KNOWLEDGE_BASE):
            continue
        doc = KNOWLEDGE_BASE[idx]
        results.append({
            'id': doc['id'],
            'title': doc['title'],
            'content': doc['content'],
            'score': float(score)
        })
    return results

# Quick retrieval test
print('Retrieval test for "How to become a seller"')
print(retrieve_docs('How do I create a seller account on Shoplite?', top_k=3))

# %%
# Cell 7: Load tokenizer & model (LLM)
# NOTE: Loading Llama 3.1 8B may require specific repo access and hardware. The code below shows an example
# using the Hugging Face transformers API. If you use a different runtime (llama.cpp, ggml) adapt accordingly.

MODEL_NAME = 'meta-llama/Llama-3.1-8B-Instruct'  # Replace if unavailable
print('Attempting to load model:', MODEL_NAME)

use_cuda = torch.cuda.is_available()
print('CUDA available:', use_cuda)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# For large models, use load_in_8bit / device_map etc. Here's a conservative load:
try:
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16 if use_cuda else torch.float32,
        device_map='auto' if use_cuda else None,
        low_cpu_mem_usage=True,
    )
    model.eval()
    print('Model loaded successfully.')
except Exception as e:
    print('Model load failed. You may need to switch to a smaller model or configure access tokens. Exception:', e)
    model = None

# %%
# Cell 8: Prompt formatting and generation

TEMPERATURE = 0.0
MAX_TOKENS = 512


def build_prompt_from_retrieval(query: str, retrieved_docs: List[Dict[str, Any]], prompt_template: Dict[str, Any] = None):
    # Use base template by default
    if prompt_template is None:
        prompt_template = PROMPTS['base_retrieval_prompt']

    role = prompt_template['role']
    goal = prompt_template['goal']
    context_guidelines = '\n'.join(f'- {l}' for l in prompt_template.get('context_guidelines', []))
    response_format = prompt_template.get('response_format', '')

    docs_text = '\n\n---\n\n'.join([f"Title: {d['title']}\nContent:\n{d['content']}" for d in retrieved_docs])

    prompt = f"{role}\nGoal: {goal}\n\nContext Guidelines:\n{context_guidelines}\n\nRetrieved Documents:\n{docs_text}\n\nUser Query: {query}\n\n{response_format}\n"
    return prompt


def generate_response(query: str, top_k: int = 3, temperature: float = TEMPERATURE, max_tokens: int = MAX_TOKENS):
    retrieved = retrieve_docs(query, top_k=top_k)
    prompt = build_prompt_from_retrieval(query, retrieved, PROMPTS.get('multi_doc_synthesis'))

    if model is None:
        # Fallback: simple heuristic answer using retrieved docs
        answer = 'I could not load the LLM in this environment. Here are the top retrieved documents and their summaries:\n\n'
        for d in retrieved:
            answer += f"- {d['title']} (score={d['score']:.3f})\n"
        return {"answer": answer, "sources": [d['title'] for d in retrieved], "confidence": "Low"}

    input_ids = tokenizer(prompt, return_tensors='pt').input_ids
    if torch.cuda.is_available():
        input_ids = input_ids.cuda()
        model.to('cuda')

    with torch.no_grad():
        outputs = model.generate(
            input_ids,
            do_sample=(temperature > 0),
            temperature=temperature,
            max_new_tokens=max_tokens,
            top_p=0.95,
            eos_token_id=tokenizer.eos_token_id,
        )
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Try to extract structured output after the prompt
    # If model echoes prompt, remove the prompt portion
    if prompt in text:
        text = text.split(prompt)[-1].strip()

    return {"answer": text, "sources": [d['title'] for d in retrieved], "confidence": "Medium"}

# %%
# Cell 9: Flask app exposing endpoints
app = Flask(__name__)

@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        'status': 'ok',
        'model_loaded': model is not None,
        'num_docs': len(KNOWLEDGE_BASE)
    })

@app.route('/ping', methods=['POST'])
def ping():
    data = request.json or {}
    text = data.get('text', 'Hello')
    # Simple echo or direct LLM call
    if model is None:
        return jsonify({'reply': f'Model not loaded. Echo: {text}'})
    prompt = f"{PROMPTS['base_retrieval_prompt']['role']}\nUser: {text}\nRespond briefly."
    input_ids = tokenizer(prompt, return_tensors='pt').input_ids
    if torch.cuda.is_available():
        input_ids = input_ids.cuda()
    with torch.no_grad():
        outputs = model.generate(input_ids, max_new_tokens=128)
    reply = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if prompt in reply:
        reply = reply.split(prompt)[-1].strip()
    return jsonify({'reply': reply})

@app.route('/chat', methods=['POST'])
def chat():
    data = request.json or {}
    query = data.get('query')
    top_k = int(data.get('top_k', 3))
    if not query:
        return jsonify({'error': 'missing query'}), 400
    try:
        result = generate_response(query, top_k=top_k)
        return jsonify(result)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# Run Flask in a background thread so the notebook keeps running

def run_flask():
    app.run(host='0.0.0.0', port=5000)

# %%
# Cell 10: ngrok setup (prompts for token!)
print('\n=== NGROK TUNNEL SETUP ===')
ngrok_token = input('Enter your ngrok authtoken (paste it here): ').strip()
if ngrok_token:
    try:
        ngrok.set_auth_token(ngrok_token)
        public_url = ngrok.connect(5000)
        print('ngrok tunnel created at:', public_url)
    except Exception as e:
        print('Failed to create ngrok tunnel:', e)
        public_url = None
else:
    print('No ngrok token provided; remember to set up a tunnel separately.')
    public_url = None

# Start Flask app in background thread
thread = threading.Thread(target=run_flask, daemon=True)
thread.start()

print('Flask app launched on port 5000. If ngrok is connected, external URL is above.')

# %%
# Cell 11: Quick tests (run these after model loads and ngrok is active)
if public_url:
    print('\nPublic URL:', public_url)
    print('Example curl:')
    print(f"curl -X POST {public_url}/chat -H 'Content-Type: application/json' -d '{"{'"}query{"'}": "How do I create a seller account on Shoplite?"}'")

print('\nLocal health check:')
try:
    import requests
    r = requests.get('http://127.0.0.1:5000/health')
    print('Health:', r.json())
except Exception as e:
    print('Local health check failed:', e)

# %%
# Cell 12: Save index and embeddings to disk (optional)
# This helps with restarts in Colab; files will be stored in the runtime.
np.save('doc_embeddings.npy', doc_embeddings)
with open('kb.json', 'w', encoding='utf-8') as f:
    json.dump(KNOWLEDGE_BASE, f, ensure_ascii=False, indent=2)
print('Saved doc_embeddings.npy and kb.json in current directory.')

# %%
# Cell 13: Notes and troubleshooting
print('''
Notes:
- If the model fails to load due to size/access constraints, switch MODEL_NAME to a smaller open-source model such as 'meta-llama/Llama-2-7b' or a community LLM available to you.
- For constrained Colab GPU (e.g., 12GB) you may need to use 4-bit/8-bit quantization or offload to CPU. Consider using bitsandbytes + accelerate for 8-bit loading.
- Always enter your ngrok authtoken via the input prompt; do not hardcode it.
- This notebook is self-contained: the knowledge base and prompts are embedded above.
''')

