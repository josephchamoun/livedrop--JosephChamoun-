{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc6rDVZgnLDK"
      },
      "source": [
        "# LLM Deployment for Shoplite (Colab-ready)\n",
        "\n",
        "This notebook is self-contained. It embeds the Shoplite knowledge base, builds a FAISS index with `sentence-transformers`, loads an open-source LLM (Llama 3.1 8B by default), exposes a Flask API (`/chat`, `/ping`, `/health`) and uses `pyngrok` to create a public tunnel.\n",
        "\n",
        "**IMPORTANT**: Instructors must supply their ngrok authtoken via an `input()` prompt at runtime. Do NOT hardcode tokens. If the model cannot be loaded in Colab due to size or access restrictions, follow the troubleshooting notes in the last cell to switch to a smaller model or use quantized loading."
      ],
      "id": "lc6rDVZgnLDK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RGZOzzgnLDQ",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4913d88d-74da-40b3-ddae-e4f17df907f0"
      },
      "source": [
        "# Cell 1: Install dependencies\n",
        "# Run this cell in Colab (ensure GPU runtime selected: Runtime -> Change runtime type -> GPU)\n",
        "!pip install --quiet --upgrade pip\n",
        "!pip install --quiet transformers accelerate bitsandbytes safetensors sentence-transformers faiss-cpu flask pyngrok uvicorn gunicorn -U\n",
        "!pip install flask-cors\n",
        "\n",
        "# bitsandbytes is used for 8-bit loading/quantization; accelerate helps with device mapping\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.6/1.8 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flask-cors\n",
            "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.12/dist-packages (from flask-cors) (3.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.12/dist-packages (from flask-cors) (3.1.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (8.3.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (3.0.3)\n",
            "Downloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-6.0.1\n"
          ]
        }
      ],
      "id": "6RGZOzzgnLDQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuEInpzSnLDT"
      },
      "source": [
        "# Cell 2: Imports and utilities\n",
        "import os, time, json, threading\n",
        "from typing import List, Dict, Any\n",
        "from flask import Flask, request, jsonify\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Helper to safely serialize numpy arrays to JSON\n",
        "class NpEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return super().default(obj)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "fuEInpzSnLDT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "effDC2QanLDU"
      },
      "source": [
        "# Cell 3: Knowledge Base (embedded)\n",
        "KNOWLEDGE_BASE = [\n",
        "    {\"id\": \"doc1\", \"title\": \"Shoplite User Registration and Account Management\", \"content\": \"To create a Shoplite account, users must visit the registration page and provide a valid email address, password, and basic profile information. Email verification is required within 24 hours. Users can choose between: - Buyer accounts (free) - Seller accounts (requires business verification and tax information). Account Management Features: Update personal information, Change passwords, Set security questions, Manage notification preferences, Deactivate accounts (requires email confirmation; may affect active orders/subscriptions). Buyer Access: product browsing, purchasing, order tracking, reviews. Seller Access: seller dashboard, inventory management, order processing, analytics. Security Measures: two-factor authentication recommended, password recovery via email and phone verification.\"},\n",
        "    {\"id\": \"doc2\", \"title\": \"Shoplite Product Search and Filtering Features\", \"content\": \"Shoplite provides a powerful search engine: Search Capabilities: keyword queries, category selection, brand filters. Filtering Options: price range, rating, availability, seller location, shipping speed, promotions, eco-friendly options. Features: Autocomplete suggestions, Spelling correction, Save searches & alerts, Faceted navigation, Optimized for large catalogs with real-time indexing, Mobile responsive interface.\"},\n",
        "    {\"id\": \"doc3\", \"title\": \"Shoplite Shopping Cart and Checkout Process\", \"content\": \"Add multiple items from different sellers; Review quantities, apply promo codes/gift cards; Cart preserved across sessions for logged-in users. Checkout Steps: 1. Shipping selection (standard, expedited, same-day) 2. Payment selection (credit/debit cards, digital wallets, cash-on-delivery) 3. Order confirmation. Security & Processing: PCI-DSS compliant payment gateways, Real-time stock updates, Order confirmation emails with tracking, Seller notifications for new orders, Integrated returns and refunds system.\"},\n",
        "    {\"id\": \"doc4\", \"title\": \"Shoplite Payment Methods and Security\", \"content\": \"Accepted Payment Methods: credit/debit cards, PayPal, Apple Pay, Google Pay, local solutions. Security Measures: SSL encryption, PCI-DSS compliance, fraud detection, two-factor authentication, sensitive info encrypted in transit and at rest. Other Features: digital wallet integration, structured dispute/chargeback process, seller payments after order confirmation.\"},\n",
        "    {\"id\": \"doc5\", \"title\": \"Shoplite Order Tracking and Delivery\", \"content\": \"Real-time tracking with confirmation emails and unique tracking number. Stages: confirmed -> processing -> shipped -> in transit -> delivered. Delivery modification requests (seller approval required). International shipments display customs/import duties. Optimized logistics with estimated arrival and delay notifications. Support assistance for lost/delayed packages.\"},\n",
        "    {\"id\": \"doc6\", \"title\": \"Shoplite Return and Refund Policies\", \"content\": \"Return Period: typically 30 days from delivery. Process: select order/item, specify reason, use prepaid label if eligible. Refunds: processed in 5â€“7 business days to original payment method. Digital/personalized items may have exceptions. Automated order status updates. Sellers must comply with policies to maintain ratings. Dispute resolution available.\"},\n",
        "    {\"id\": \"doc7\", \"title\": \"Shoplite Product Reviews and Ratings\", \"content\": \"Buyers rate products on a five-star scale and leave comments. Reviews moderated for compliance. Sellers can respond to reviews. Ratings influence search ranking. Verified purchase badges for authenticity. Aggregate ratings provided. Review analytics available for sellers.\"},\n",
        "    {\"id\": \"doc8\", \"title\": \"Shoplite Seller Account Setup and Management\", \"content\": \"Create seller account with business documents and tax verification. Seller Dashboard: inventory management, order processing, sales analytics. Product listing via individual or bulk upload (CSV/API). Profile customization: branding, policies, shipping, returns. Notifications: new orders, low stock, inquiries. Pricing, promotions, and shipping fee management. Performance metrics tracked; third-party integrations supported.\"},\n",
        "    {\"id\": \"doc9\", \"title\": \"Shoplite Inventory Management for Sellers\", \"content\": \"Track stock levels, reorder thresholds, and availability in real-time. Low-stock alerts. Bulk imports supported. Variants (size, color, bundle) supported. Inventory reports for trends and seasonal demand. Manage warehouses and shipping locations.\"},\n",
        "    {\"id\": \"doc10\", \"title\": \"Shoplite Commission and Fee Structure\", \"content\": \"Commission fees per product category. Additional fees: premium listings, promotions, special services. Transparent notifications in dashboard. Payments made after commission deduction (weekly/bi-weekly). Transaction reports available. Pricing guidance provided.\"},\n",
        "    {\"id\": \"doc11\", \"title\": \"Shoplite Customer Support Procedures\", \"content\": \"Support via live chat, email, phone, and AI chatbot (24/7). Ticket categorization: orders, payments, returns, technical, account management. Unique tracking IDs. Backend integration for order/payment info. Dedicated seller support channel. Help center with guides, FAQs, videos. Fast, transparent, fair resolution.\"},\n",
        "    {\"id\": \"doc12\", \"title\": \"Shoplite Mobile App Features\", \"content\": \"iOS & Android support. Browse, filter, add to cart, purchase. Push notifications for promotions/order updates. Barcode scanning and QR code payments. Mobile wallets, fingerprint, Face ID login. Seller management on-the-go. Offline caching for previously loaded content. Intuitive, responsive, accessible interface.\"},\n",
        "    {\"id\": \"doc13\", \"title\": \"Shoplite API Documentation for Developers\", \"content\": \"RESTful API endpoints: product catalog, orders, accounts, inventory. OAuth 2.0 authentication. Rate limiting (higher for verified partners). Detailed docs: request/response, parameters, error codes. Webhooks for real-time events. Sandbox environment for testing. Versioned API with backward-compatible updates.\"},\n",
        "    {\"id\": \"doc14\", \"title\": \"Shoplite Security and Privacy Policies\", \"content\": \"Data Protection: TLS encryption, AES-256 at rest, authorized access. Two-factor authentication & strong passwords. GDPR & CCPA compliance. Security monitoring for suspicious activity. Clear privacy policies: data collection, usage, third-party sharing. Policy change notifications; user control over data.\"},\n",
        "    {\"id\": \"doc15\", \"title\": \"Shoplite Promotional Codes and Discounts\", \"content\": \"Sellers create promotions: discount codes, seasonal sales, bundle offers. Code types: percentage, fixed, conditional. Start/end dates, usage limits, minimum purchase configurable. Automatic verification at checkout. Analytics: redemption, revenue, engagement. User notifications for active promotions. Special events highlighted on homepage/app. Compliance with platform policies.\"}\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "effDC2QanLDU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox97pQV-nLDW"
      },
      "source": [
        "# Cell 4: Prompts embedded as Python dict (converted from assistant-prompts.yml)\n",
        "PROMPTS = {\n",
        "    \"version\": \"1.0\",\n",
        "    \"created\": \"2025-09-23\",\n",
        "    \"author\": \"Joseph Chamoun\",\n",
        "    \"base_retrieval_prompt\": {\n",
        "        \"role\": \"You are a helpful Shoplite customer service assistant.\",\n",
        "        \"goal\": \"Provide accurate answers using only the provided Shoplite documentation.\",\n",
        "        \"context_guidelines\": [\"Use only information from the provided document snippets\", \"Cite specific documents when possible\"],\n",
        "        \"response_format\": \"Answer: [Your response based on context]\\nSources: [List document titles referenced]\\n\",\n",
        "    },\n",
        "    \"multi_doc_synthesis\": {\n",
        "        \"role\": \"You are an expert Shoplite support agent who synthesizes multiple documents.\",\n",
        "        \"goal\": \"Combine information from multiple retrieved documents to create a concise, accurate answer.\",\n",
        "        \"context_guidelines\": [\"State which documents you used\", \"When information conflicts, show both options and recommend the safer/default one\"],\n",
        "        \"response_format\": \"Answer: [Synthesis]\\nSources: [Doc titles]\\nConfidence: [High|Medium|Low]\\n\",\n",
        "    },\n",
        "    \"refusal_when_no_context\": {\n",
        "        \"role\": \"You are a safety-conscious assistant.\",\n",
        "        \"goal\": \"Refuse to answer if no relevant context is found in the knowledge base and ask for clarification or external data.\",\n",
        "        \"context_guidelines\": [\"If top retrieved documents have low similarity (< threshold), return a refusal\", \"Suggest the user provide more details or check the Shoplite help center\"],\n",
        "        \"response_format\": \"Answer: I don't have enough information in the Shoplite docs to answer that. Please provide more details or check [Help Center].\\n\",\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "Ox97pQV-nLDW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9zwZ3MlnLDX"
      },
      "source": [
        "# Cell 5: Build embeddings and FAISS index\n",
        "EMBED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "print('Loading embedding model:', EMBED_MODEL_NAME)\n",
        "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "\n",
        "DOCUMENT_TEXTS = [d['title'] + '\\n\\n' + d['content'] for d in KNOWLEDGE_BASE]\n",
        "DOC_IDS = [d['id'] for d in KNOWLEDGE_BASE]\n",
        "\n",
        "print('Encoding documents...')\n",
        "doc_embeddings = embed_model.encode(DOCUMENT_TEXTS, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "def normalize_embeddings(embs):\n",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
        "    return embs / np.clip(norms, a_min=1e-10, a_max=None)\n",
        "\n",
        "doc_embeddings = normalize_embeddings(doc_embeddings)\n",
        "d = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "index.add(doc_embeddings)\n",
        "print(f'FAISS index created with {index.ntotal} vectors (dim={d})')\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "K9zwZ3MlnLDX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoLI92xvnLDX"
      },
      "source": [
        "# Cell 6: Retrieval functions\n",
        "def retrieve_docs(query: str, top_k: int = 3):\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
        "    q_emb = normalize_embeddings(q_emb)\n",
        "    scores, indices = index.search(q_emb, top_k)\n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], indices[0]):\n",
        "        if idx < 0 or idx >= len(KNOWLEDGE_BASE):\n",
        "            continue\n",
        "        doc = KNOWLEDGE_BASE[idx]\n",
        "        results.append({\n",
        "            'id': doc['id'],\n",
        "            'title': doc['title'],\n",
        "            'content': doc['content'],\n",
        "            'score': float(score)\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# Quick test (optional)\n",
        "print(retrieve_docs('How do I create a seller account on Shoplite?', top_k=3))\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "yoLI92xvnLDX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6.5 (REPLACE): Hugging Face Login (runtime input; no hardcoded token)\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = input(\"ðŸ”‘ Enter your Hugging Face token (paste it here; will not be saved in the notebook): \").strip()\n",
        "if not hf_token:\n",
        "    print(\"No token provided. Attempts to load private models may fail. Proceeding without login.\")\n",
        "else:\n",
        "    try:\n",
        "        login(hf_token)\n",
        "        print(\"Hugging Face login successful.\")\n",
        "    except Exception as e:\n",
        "        print(\"Hugging Face login failed:\", e)\n",
        "        print(\"If you can't load protected models, try switching to a publicly available model.\")\n"
      ],
      "metadata": {
        "id": "O6bNRdRwn0lq"
      },
      "id": "O6bNRdRwn0lq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfyCXmAPnLDY"
      },
      "source": [
        "# Cell 7 (REPLACE): Model loading with modern quantization config + safe fallbacks\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"  # preferred (may require HF access)\n",
        "FALLBACK_MODEL = \"tiiuae/falcon-7b-instruct\"     # a smaller public instruct model used as fallback\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(\"Attempting to load model:\", MODEL_NAME)\n",
        "print(\"CUDA available:\", use_cuda)\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "\n",
        "def try_load_model(model_name, quant_config=None, dtype=torch.float16):\n",
        "    \"\"\"Attempt to load the model with optional quantization config.\"\"\"\n",
        "    try:\n",
        "        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        if quant_config is not None:\n",
        "            mdl = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=\"auto\" if use_cuda else None,\n",
        "                quantization_config=quant_config,\n",
        "                torch_dtype=dtype,\n",
        "                low_cpu_mem_usage=True,\n",
        "            )\n",
        "        else:\n",
        "            mdl = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=\"auto\" if use_cuda else None,\n",
        "                torch_dtype=dtype,\n",
        "                low_cpu_mem_usage=True,\n",
        "            )\n",
        "        mdl.eval()\n",
        "        return tok, mdl\n",
        "    except Exception as e:\n",
        "        print(f\"Load failed for {model_name}: {repr(e)}\")\n",
        "        return None, None\n",
        "\n",
        "# Prepare a BitsAndBytesConfig for 8-bit (modern API)\n",
        "try:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,          # prefer 8-bit quantization when available\n",
        "        llm_int8_threshold=6.0      # heuristic threshold (tweak if needed)\n",
        "    )\n",
        "except Exception:\n",
        "    # In case BitsAndBytesConfig import signature differs on some versions\n",
        "    bnb_config = None\n",
        "\n",
        "# 1) Try to load preferred model with quantization\n",
        "if bnb_config is not None:\n",
        "    print(\"Trying quantized load with BitsAndBytesConfig...\")\n",
        "    tokenizer, model = try_load_model(MODEL_NAME, quant_config=bnb_config, dtype=torch.float16)\n",
        "else:\n",
        "    print(\"BitsAndBytesConfig not available; attempting standard/float16 load...\")\n",
        "    tokenizer, model = try_load_model(MODEL_NAME, quant_config=None, dtype=torch.float16)\n",
        "\n",
        "# 2) Fallback: try without quant or try the fallback model\n",
        "if model is None:\n",
        "    print(\"Primary model load failed. Trying fallback public model:\", FALLBACK_MODEL)\n",
        "    tokenizer, model = try_load_model(FALLBACK_MODEL, quant_config=bnb_config, dtype=torch.float16)\n",
        "    if model:\n",
        "        print(\"Fallback model loaded:\", FALLBACK_MODEL)\n",
        "    else:\n",
        "        print(\"Fallback model also failed. The notebook will still run retrieval, but LLM responses will be disabled.\")\n",
        "        model = None\n",
        "        tokenizer = None\n",
        "\n",
        "# Final status\n",
        "if model is not None and tokenizer is not None:\n",
        "    print(\"Model and tokenizer loaded successfully.\")\n",
        "else:\n",
        "    print(\"No model loaded. Generation endpoints will return retrieval-only responses.\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "tfyCXmAPnLDY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyO4x8ZQnLDY"
      },
      "source": [
        "# Cell 8: Final optimized version with proper text extraction\n",
        "import torch\n",
        "\n",
        "TEMPERATURE = 0.7\n",
        "MAX_TOKENS = 120\n",
        "SIMILARITY_THRESHOLD = 0.25\n",
        "\n",
        "def build_prompt_from_retrieval(query: str, retrieved_docs: List[Dict[str, Any]]):\n",
        "    \"\"\"Builds prompt using top 5 documents.\"\"\"\n",
        "    docs_text = \"\"\n",
        "    for i, doc in enumerate(retrieved_docs[:5], 1):\n",
        "        content = doc['content'][:200] + \"...\" if len(doc['content']) > 200 else doc['content']\n",
        "        docs_text += f\"\\n[Document {i}: {doc['title']}]\\n{content}\\n\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are a helpful Shoplite customer service assistant.\\n\\n\"\n",
        "        f\"Use these documents to answer:{docs_text}\\n\\n\"\n",
        "        f\"Question: {query}\\n\"\n",
        "        f\"Answer in 2-3 sentences:\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def generate_response(query: str, top_k: int = 5, temperature: float = TEMPERATURE, max_tokens: int = MAX_TOKENS, debug: bool = False):\n",
        "    \"\"\"Generate response with proper text extraction.\"\"\"\n",
        "\n",
        "    query_lower = query.lower().strip()\n",
        "\n",
        "    # Greetings\n",
        "    if any(greeting in query_lower for greeting in [\"hi\", \"hello\", \"hey\", \"good morning\"]):\n",
        "        if \"how are you\" in query_lower:\n",
        "            return {\"answer\": \"I'm here and ready to help! How can I assist you with Shoplite today?\", \"sources\": [], \"confidence\": \"High\"}\n",
        "        return {\"answer\": \"Hello! I'm here to help you with Shoplite. What would you like to know?\", \"sources\": [], \"confidence\": \"High\"}\n",
        "\n",
        "    # Help requests\n",
        "    if query_lower in [\"help\", \"can you help\", \"can u help me\", \"help me\"]:\n",
        "        return {\"answer\": \"Of course! I can help you with Shoplite registration, orders, payments, returns, seller accounts, and more. What specific information do you need?\", \"sources\": [], \"confidence\": \"High\"}\n",
        "\n",
        "    # Retrieve documents\n",
        "    retrieved = retrieve_docs(query, top_k=top_k)\n",
        "\n",
        "    if not retrieved:\n",
        "        return {\n",
        "            \"answer\": \"I don't have information about that. I can only answer questions about Shoplite's platform, features, and services.\",\n",
        "            \"sources\": [],\n",
        "            \"confidence\": \"Low\"\n",
        "        }\n",
        "\n",
        "    top_score = max(d['score'] for d in retrieved)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Top similarity score: {top_score:.3f}\")\n",
        "\n",
        "    # Reject only truly irrelevant queries\n",
        "    if top_score < SIMILARITY_THRESHOLD:\n",
        "        return {\n",
        "            \"answer\": \"I'm sorry, that question appears to be outside my knowledge base. I can help with questions about Shoplite's registration, orders, payments, returns, seller accounts, product search, and customer support.\",\n",
        "            \"sources\": [],\n",
        "            \"confidence\": \"Low\"\n",
        "        }\n",
        "\n",
        "    # Adjusted confidence thresholds\n",
        "    if top_score >= 0.6:\n",
        "        confidence = \"High\"\n",
        "    elif top_score >= 0.3:\n",
        "        confidence = \"Medium\"\n",
        "    else:\n",
        "        confidence = \"Low\"\n",
        "\n",
        "    prompt = build_prompt_from_retrieval(query, retrieved)\n",
        "\n",
        "    if model is None:\n",
        "        answer_parts = []\n",
        "        for doc in retrieved[:3]:\n",
        "            sentences = [s.strip() + '.' for s in doc['content'].split('.') if s.strip()][:1]\n",
        "            answer_parts.extend(sentences)\n",
        "        answer = ' '.join(answer_parts[:3])\n",
        "        return {\"answer\": answer, \"sources\": [d['title'] for d in retrieved[:3]], \"confidence\": confidence}\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1536)\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            max_new_tokens=max_tokens,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.3,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "    raw_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Raw generated text: {raw_text[:200]}\")\n",
        "\n",
        "    # Extract meaningful content - take first 3 sentences\n",
        "    sentences = []\n",
        "    current = \"\"\n",
        "\n",
        "    for char in raw_text:\n",
        "        current += char\n",
        "        # End of sentence markers\n",
        "        if char in '.!?' and len(current.strip()) > 15:\n",
        "            sentences.append(current.strip())\n",
        "            current = \"\"\n",
        "            if len(sentences) >= 3:\n",
        "                break\n",
        "\n",
        "    # Join first 2-3 sentences\n",
        "    text = ' '.join(sentences[:3])\n",
        "\n",
        "    # Remove trailing incomplete sentence\n",
        "    if text and not any(text.endswith(p) for p in ['.', '!', '?']):\n",
        "        last_period = text.rfind('.')\n",
        "        if last_period > 0:\n",
        "            text = text[:last_period + 1]\n",
        "\n",
        "    # If still empty or too short, use extractive approach\n",
        "    if len(text) < 20:\n",
        "        top_doc = retrieved[0]\n",
        "        content_sentences = [s.strip() + '.' for s in top_doc['content'].split('.') if len(s.strip()) > 20][:2]\n",
        "        text = ' '.join(content_sentences)\n",
        "\n",
        "    return {\n",
        "        \"answer\": text,\n",
        "        \"sources\": [d['title'] for d in retrieved[:3]],\n",
        "        \"confidence\": confidence\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "pyO4x8ZQnLDY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kOWRtSjnLDZ"
      },
      "source": [
        "# Cell 9 (UPDATED): Flask server with /generate endpoint for Week 5\n",
        "from flask import Flask, request, jsonify\n",
        "import threading\n",
        "from flask_cors import CORS\n",
        "\n",
        "app = Flask(__name__)\n",
        "# Allow only your frontend origin\n",
        "# In your Colab, update this line:\n",
        "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})  # Allow all origins for development\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({'status': 'ok', 'model_loaded': model is not None, 'num_docs': len(KNOWLEDGE_BASE)})\n",
        "\n",
        "@app.route('/ping', methods=['POST'])\n",
        "def ping():\n",
        "    data = request.json or {}\n",
        "    text = data.get('text', 'Hello')\n",
        "    return jsonify({'reply': f'Pong: {text}'})\n",
        "\n",
        "# ===== WEEK 3 ENDPOINT (Keep existing) =====\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    \"\"\"Week 3 RAG endpoint - uses retrieval + generation\"\"\"\n",
        "    data = request.json or {}\n",
        "    query = data.get('query')\n",
        "    top_k = int(data.get('top_k', 3))\n",
        "    debug = data.get('debug', False)\n",
        "\n",
        "    if not query:\n",
        "        return jsonify({'error': 'missing query'}), 400\n",
        "\n",
        "    try:\n",
        "        result = generate_response(query, top_k=top_k, debug=debug)\n",
        "        return jsonify(result)\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "\n",
        "# ===== NEW WEEK 5 ENDPOINT =====\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate():\n",
        "    \"\"\"\n",
        "    Week 5 simple text generation endpoint\n",
        "    No retrieval, no RAG - just pure text completion\n",
        "    Backend does its own grounding before calling this\n",
        "    \"\"\"\n",
        "    data = request.json or {}\n",
        "    prompt = data.get('prompt')\n",
        "    max_tokens = int(data.get('max_tokens', 300))\n",
        "    temperature = float(data.get('temperature', 0.7))\n",
        "\n",
        "    if not prompt:\n",
        "        return jsonify({'error': 'missing prompt'}), 400\n",
        "\n",
        "    # If model not loaded, return a fallback response\n",
        "    if model is None or tokenizer is None:\n",
        "        return jsonify({\n",
        "            'text': \"I'm currently unavailable. Please try again later.\",\n",
        "            'model_status': 'unavailable'\n",
        "        })\n",
        "\n",
        "    try:\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1024)\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                do_sample=True,\n",
        "                temperature=temperature,\n",
        "                max_new_tokens=max_tokens,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.2,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        # Decode only the new tokens (exclude input prompt)\n",
        "        generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "        text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "        # Clean up the response - take first few complete sentences\n",
        "        sentences = []\n",
        "        current = \"\"\n",
        "        for char in text:\n",
        "            current += char\n",
        "            if char in '.!?' and len(current.strip()) > 10:\n",
        "                sentences.append(current.strip())\n",
        "                current = \"\"\n",
        "                if len(sentences) >= 4:  # Max 4 sentences\n",
        "                    break\n",
        "\n",
        "        # Join sentences\n",
        "        final_text = ' '.join(sentences[:4])\n",
        "\n",
        "        # If no complete sentences, just return truncated text\n",
        "        if not final_text:\n",
        "            final_text = text[:500] if len(text) > 500 else text\n",
        "\n",
        "        return jsonify({\n",
        "            'text': final_text,\n",
        "            'model_status': 'ok'\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Generation error: {str(e)}\")\n",
        "        return jsonify({\n",
        "            'error': str(e),\n",
        "            'text': \"Sorry, I encountered an error generating a response.\"\n",
        "        }), 500\n",
        "\n",
        "\n",
        "# Run Flask in background thread\n",
        "def run_flask():\n",
        "    app.run(host='0.0.0.0', port=5000, threaded=True)\n",
        "\n",
        "thread = threading.Thread(target=run_flask, daemon=True)\n",
        "thread.start()\n",
        "print('Flask server started (background thread).')\n",
        "print('Available endpoints:')\n",
        "print('  - POST /chat      (Week 3 RAG)')\n",
        "print('  - POST /generate  (Week 5 simple generation)')\n",
        "print('  - GET  /health')\n",
        "print('  - POST /ping')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8kOWRtSjnLDZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y180W8OYnLDZ"
      },
      "source": [
        "# Cell 10: ngrok tunnel setup (prompt for authtoken)\n",
        "\n",
        "!pkill -f ngrok\n",
        "\n",
        "print('\\n=== NGROK TUNNEL SETUP ===')\n",
        "ngrok_token = input('Enter your ngrok authtoken (paste it here): ').strip()\n",
        "if ngrok_token:\n",
        "    try:\n",
        "        ngrok.set_auth_token(ngrok_token)\n",
        "        public_url = ngrok.connect(5000)\n",
        "        print('ngrok tunnel created at:', public_url)\n",
        "    except Exception as e:\n",
        "        print('Failed to create ngrok tunnel:', e)\n",
        "        public_url = None\n",
        "else:\n",
        "    print('No ngrok token provided; remember to set up a tunnel separately.')\n",
        "    public_url = None\n",
        "\n",
        "print('If public_url is not None, use it to call the /chat endpoint from outside Colab.')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "Y180W8OYnLDZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRnXFizrnLDb"
      },
      "source": [
        "# Cell 11: Quick test examples (run after ngrok created and model loaded)\n",
        "print('Local health check:')\n",
        "try:\n",
        "    import requests\n",
        "    r = requests.get('http://127.0.0.1:5000/health', timeout=5)\n",
        "    print('Health:', r.json())\n",
        "except Exception as e:\n",
        "    print('Local health check failed:', e)\n",
        "\n",
        "if 'public_url' in globals() and public_url:\n",
        "    print(f'Call this externally: POST {public_url}/chat with JSON {{\"query\": \"How do I create a seller account on Shoplite?\"}}')\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "BRnXFizrnLDb"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "r = requests.post(\n",
        "    \"http://127.0.0.1:5000/chat\",\n",
        "    json={\"query\": \"can u help me, how to register?\"}\n",
        ")\n",
        "\n",
        "print(r.json())\n"
      ],
      "metadata": {
        "id": "TQ3gzBBcsDAa"
      },
      "id": "TQ3gzBBcsDAa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Test the /generate endpoint locally\n",
        "response = requests.post(\n",
        "    \"http://127.0.0.1:5000/generate\",\n",
        "    json={\"prompt\": \"Say hello\", \"max_tokens\": 50}\n",
        ")\n",
        "\n",
        "print(\"Status:\", response.status_code)\n",
        "print(\"Response:\", response.json())"
      ],
      "metadata": {
        "id": "Lp45RzzaGAI3",
        "collapsed": true
      },
      "id": "Lp45RzzaGAI3",
      "execution_count": null,
      "outputs": []
    }
  ]
}
